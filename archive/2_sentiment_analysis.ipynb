{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "#other needed packages\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Set JAVA_HOME for PySpark\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@17'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"stock market preds\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6016d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to import data, run SQL from file and save back to file\n",
    "# function to import and clean columns\n",
    "def import_csv_to_table(table_name, file, format_cols):\n",
    "\n",
    "    #read source files\n",
    "    df = spark.read.csv(file, header=True, quote=\"\\\"\",\n",
    "                        escape=\"\\\"\", multiLine=True, inferSchema=True)\n",
    "\n",
    "    #clean column names\n",
    "    if format_cols:\n",
    "        cols_formatted = [re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", col_name).lower().replace(\" \", \"_\") for col_name in df.columns]\n",
    "        df = df.toDF(*cols_formatted)\n",
    "\n",
    "    # create SQL view\n",
    "    df.createOrReplaceTempView(f\"{table_name}\")\n",
    "    return df\n",
    "\n",
    "#run a SQL step\n",
    "def sql_step(file):\n",
    "    with open(file, 'r', encoding='utf-8') as file:\n",
    "        sql_text = file.read()\n",
    "    results = spark.sql(sql_text)\n",
    "    return results\n",
    "\n",
    "#run SQL and view output inline\n",
    "def run_sql(file, rowstoshow, print_sql):\n",
    "    with open(file, 'r', encoding='utf-8') as file:\n",
    "        sql_text = file.read()\n",
    "    results = spark.sql(sql_text)\n",
    "    if print_sql == True: print(sql_text)\n",
    "    results.show(rowstoshow, truncate=False)\n",
    "\n",
    "# export data frame to csv\n",
    "def export_csv(df, output_dir, final_file_name):\n",
    "    df.coalesce(1).write.csv(output_dir, header=True, mode=\"overwrite\")\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.startswith(\"part-\") and file.endswith(\".csv\"):\n",
    "            part_file_path = os.path.join(output_dir, file)\n",
    "            break\n",
    "    if part_file_path:\n",
    "        os.rename(part_file_path, os.path.join(output_dir, final_file_name))\n",
    "        print(f\"CSV saved as: {final_file_name}\")\n",
    "    else:\n",
    "        print(\"Error: Part file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = import_csv_to_table(\"news\", \"raw_data/news_data.csv\", False)\n",
    "stocks = import_csv_to_table(\"stocks\", \"raw_data/stock_data.csv\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = sql_step(\"sql/sentiment_data_prep.sql\")\n",
    "feature_set.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas and create 80/20 train/test split (no article_id leakage)\n",
    "import pandas as pd\n",
    "df = feature_set.toPandas()\n",
    "unique_ids = df['news_article_id'].unique()\n",
    "train_ids = pd.Series(unique_ids).sample(frac=0.8, random_state=42).values\n",
    "train_df, test_df = df[df['news_article_id'].isin(train_ids)], df[~df['news_article_id'].isin(train_ids)]\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('api_keys.json', 'r') as file:\n",
    "    # Code to load the data goes here\n",
    "    api_keys = json.load(file)\n",
    "\n",
    "HUGGINGFACE_KEY = api_keys['HUGGINGFACE_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# FinGPT-Forecaster Integration\n",
    "# ==========================================\n",
    "# Note: Using Apple Silicon MPS for acceleration\n",
    "# \n",
    "# PREREQUISITES:\n",
    "# 1. Request access to Llama-2 at: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "# 2. Run: huggingface-cli login (in terminal) with your HF token\n",
    "#    OR set token below\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== HUGGING FACE LOGIN ==========\n",
    "login(token=HUGGINGFACE_KEY)\n",
    "\n",
    "# Check MPS availability and set device\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(f\"✅ MPS (Metal Performance Shaders) is available!\")\n",
    "    print(f\"   Using device: {mps_device}\")\n",
    "else:\n",
    "    mps_device = torch.device(\"cpu\")\n",
    "    print(\"⚠️ MPS not available, falling back to CPU\")\n",
    "\n",
    "# Load FinGPT-Forecaster model with MPS support\n",
    "print(\"\\nLoading FinGPT-Forecaster model...\")\n",
    "print(\"(Using float16 to reduce memory - requires ~14GB RAM)\\n\")\n",
    "\n",
    "# Clear any cached memory first\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Load base model without device_map to avoid PEFT compatibility issues\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "forecaster_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "\n",
    "# Load PEFT adapter with legacy mode for older adapters\n",
    "forecaster_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    'FinGPT/fingpt-forecaster_dow30_llama2-7b_lora',\n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "# Move to MPS after loading\n",
    "forecaster_model = forecaster_model.to(mps_device)\n",
    "forecaster_model = forecaster_model.eval()\n",
    "\n",
    "print(f\"\\n✅ Model loaded successfully!\")\n",
    "print(f\"   Model dtype: {next(forecaster_model.parameters()).dtype}\")\n",
    "print(f\"   Model device: {next(forecaster_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Create Forecaster Prompt Function\n",
    "# ==========================================\n",
    "\n",
    "# System prompt for FinGPT-Forecaster\n",
    "SYSTEM_PROMPT = \"\"\"You are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and basic financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. Your answer format should be as follows:\n",
    "\n",
    "[Positive Developments]:\n",
    "1. ...\n",
    "\n",
    "[Potential Concerns]:\n",
    "1. ...\n",
    "\n",
    "[Prediction & Analysis]:\n",
    "Prediction: {Up/Down}\n",
    "Analysis: ...\n",
    "\"\"\"\n",
    "\n",
    "def create_forecaster_prompt(symbol, news_date, article_text):\n",
    "    \"\"\"\n",
    "    Create a prompt for FinGPT-Forecaster from your feature_set data\n",
    "    \"\"\"\n",
    "    # Llama-2 chat format tokens\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "[Company Symbol]: {symbol}\n",
    "[Date]: {news_date}\n",
    "\n",
    "From the news articles below, analyze {symbol}'s stock outlook:\n",
    "\n",
    "{article_text}\n",
    "\n",
    "Based on the information above, analyze the positive developments and potential concerns for {symbol}. Then predict whether the stock price will go Up or Down tomorrow.\n",
    "\"\"\"\n",
    "    \n",
    "    full_prompt = B_INST + B_SYS + SYSTEM_PROMPT + E_SYS + user_prompt + E_INST\n",
    "    return full_prompt\n",
    "\n",
    "\n",
    "def get_fingpt_prediction(symbol, news_date, article_text, max_length=2048):\n",
    "    \"\"\"\n",
    "    Get a price movement prediction from FinGPT-Forecaster\n",
    "    Returns: dict with 'prediction' (Up/Down/Neutral), 'analysis', 'raw_output'\n",
    "    \"\"\"\n",
    "    prompt = create_forecaster_prompt(symbol, news_date, article_text)\n",
    "    \n",
    "    inputs = forecaster_tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    inputs = {key: value.to(forecaster_model.device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = forecaster_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=forecaster_tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    output = forecaster_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    answer = re.sub(r'.*\\[/INST\\]\\s*', '', output, flags=re.DOTALL)\n",
    "    \n",
    "    # Extract prediction (Up/Down)\n",
    "    prediction = \"Neutral\"\n",
    "    if re.search(r'prediction[:\\s]*(up|increase|rise|positive)', answer, re.IGNORECASE):\n",
    "        prediction = \"Up\"\n",
    "    elif re.search(r'prediction[:\\s]*(down|decrease|fall|negative|decline)', answer, re.IGNORECASE):\n",
    "        prediction = \"Down\"\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'analysis': answer,\n",
    "        'raw_output': output\n",
    "    }\n",
    "\n",
    "print(\"Forecaster functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72372db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Apply FinGPT-Forecaster to feature_set\n",
    "# ==========================================\n",
    "\n",
    "# Convert to pandas if needed\n",
    "df_predictions = feature_set.toPandas()\n",
    "\n",
    "# Limit to a sample for testing (remove/increase for full dataset)\n",
    "sample_size = 10  # Adjust as needed - each prediction takes ~10-30 seconds\n",
    "df_sample = df_predictions.head(sample_size).copy()\n",
    "\n",
    "print(f\"Running FinGPT-Forecaster on {len(df_sample)} samples...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store predictions\n",
    "predictions = []\n",
    "analyses = []\n",
    "\n",
    "for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Predicting\"):\n",
    "    try:\n",
    "        result = get_fingpt_prediction(\n",
    "            symbol=row['symbol'],\n",
    "            news_date=str(row['news_date']),\n",
    "            article_text=row['daily_news_text'][:2000]  # Truncate very long articles\n",
    "        )\n",
    "        predictions.append(result['prediction'])\n",
    "        analyses.append(result['analysis'][:500])  # Store truncated analysis\n",
    "        \n",
    "        # Print progress\n",
    "        actual_direction = \"Up\" if row['percent_daily_price_change'] > 0 else \"Down\"\n",
    "        print(f\"\\n{row['symbol']} ({row['news_date']}): Predicted={result['prediction']}, Actual={actual_direction}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on {row['symbol']}: {e}\")\n",
    "        predictions.append(\"Error\")\n",
    "        analyses.append(str(e))\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_sample['fingpt_prediction'] = predictions\n",
    "df_sample['fingpt_analysis'] = analyses\n",
    "\n",
    "# Calculate actual direction for comparison\n",
    "df_sample['actual_direction'] = df_sample['percent_daily_price_change'].apply(\n",
    "    lambda x: \"Up\" if x > 0 else \"Down\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Predictions complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Evaluate FinGPT-Forecaster Performance\n",
    "# ==========================================\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Filter out errors\n",
    "df_eval = df_sample[df_sample['fingpt_prediction'] != 'Error'].copy()\n",
    "\n",
    "if len(df_eval) > 0:\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(df_eval['actual_direction'], df_eval['fingpt_prediction'])\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"FinGPT-Forecaster Evaluation Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nAccuracy: {accuracy:.2%}\")\n",
    "    print(f\"Samples evaluated: {len(df_eval)}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(df_eval['actual_direction'], df_eval['fingpt_prediction']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(df_eval['actual_direction'], df_eval['fingpt_prediction'], labels=['Up', 'Down'])\n",
    "    cm_df = pd.DataFrame(cm, index=['Actual Up', 'Actual Down'], columns=['Pred Up', 'Pred Down'])\n",
    "    print(cm_df)\n",
    "    \n",
    "    # Show sample predictions with actual results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample Predictions vs Actual:\")\n",
    "    print(\"=\"*60)\n",
    "    display_cols = ['symbol', 'news_date', 'fingpt_prediction', 'actual_direction', 'percent_daily_price_change']\n",
    "    print(df_eval[display_cols].to_string(index=False))\n",
    "else:\n",
    "    print(\"No successful predictions to evaluate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
