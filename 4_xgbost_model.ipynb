{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fccd2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/19 13:30:40 WARN Utils: Your hostname, Jeffreys-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.0.0.17 instead (on interface en0)\n",
      "26/01/19 13:30:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/19 13:30:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# pyspark packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "#other needed packages\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Set JAVA_HOME for PySpark\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@17'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"stock market preds\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb41dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to import data, run SQL from file and save back to file\n",
    "# function to import and clean columns\n",
    "def import_csv_to_table(table_name, file, format_cols):\n",
    "\n",
    "    #read source files\n",
    "    df = spark.read.csv(file, header=True, quote=\"\\\"\",\n",
    "                        escape=\"\\\"\", multiLine=True, inferSchema=True)\n",
    "\n",
    "    #clean column names\n",
    "    if format_cols:\n",
    "        cols_formatted = [re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", col_name).lower().replace(\" \", \"_\") for col_name in df.columns]\n",
    "        df = df.toDF(*cols_formatted)\n",
    "\n",
    "    # create SQL view\n",
    "    df.createOrReplaceTempView(f\"{table_name}\")\n",
    "    return df\n",
    "\n",
    "#run a SQL step\n",
    "def sql_step(file):\n",
    "    with open(file, 'r', encoding='utf-8') as file:\n",
    "        sql_text = file.read()\n",
    "    results = spark.sql(sql_text)\n",
    "    return results\n",
    "\n",
    "#run SQL and view output inline\n",
    "def run_sql(file, rowstoshow, print_sql):\n",
    "    with open(file, 'r', encoding='utf-8') as file:\n",
    "        sql_text = file.read()\n",
    "    results = spark.sql(sql_text)\n",
    "    if print_sql == True: print(sql_text)\n",
    "    results.show(rowstoshow, truncate=False)\n",
    "\n",
    "# export data frame to csv\n",
    "def export_csv(df, output_dir, final_file_name):\n",
    "    df.coalesce(1).write.csv(output_dir, header=True, mode=\"overwrite\")\n",
    "    for file in os.listdir(output_dir):\n",
    "        if file.startswith(\"part-\") and file.endswith(\".csv\"):\n",
    "            part_file_path = os.path.join(output_dir, file)\n",
    "            break\n",
    "    if part_file_path:\n",
    "        os.rename(part_file_path, os.path.join(output_dir, final_file_name))\n",
    "        print(f\"CSV saved as: {final_file_name}\")\n",
    "    else:\n",
    "        print(\"Error: Part file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables imported and SQL views created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Use absolute paths to avoid OneDrive timeout issues with Spark\n",
    "import os\n",
    "base_path = os.getcwd()\n",
    "\n",
    "# Import raw_data tables\n",
    "raw_data_path = os.path.join(base_path, \"raw_data\")\n",
    "alfred_economic_data = import_csv_to_table(\"alfred_economic_data\", os.path.join(raw_data_path, \"alfred_economic_data.csv\"), False)\n",
    "balance_sheet = import_csv_to_table(\"balance_sheet\", os.path.join(raw_data_path, \"balance_sheet.csv\"), False)\n",
    "cashflow = import_csv_to_table(\"cashflow\", os.path.join(raw_data_path, \"cashflow.csv\"), False)\n",
    "earnings_dates = import_csv_to_table(\"earnings_dates\", os.path.join(raw_data_path, \"earnings_dates.csv\"), False)\n",
    "income_statement = import_csv_to_table(\"income_statement\", os.path.join(raw_data_path, \"income_statement.csv\"), False)\n",
    "news_data = import_csv_to_table(\"news_data\", os.path.join(raw_data_path, \"news_data.csv\"), False)\n",
    "stock_data = import_csv_to_table(\"stock_data\", os.path.join(raw_data_path, \"stock_data.csv\"), False)\n",
    "tickers = import_csv_to_table(\"tickers\", os.path.join(raw_data_path, \"tickers.csv\"), False)\n",
    "\n",
    "# Import processed_data tables\n",
    "processed_data_path = os.path.join(base_path, \"processed_data\")\n",
    "finbert_news_classifications = import_csv_to_table(\"finbert_news_classifications\", os.path.join(processed_data_path, \"finbert_news_classifications.csv\"), False)\n",
    "fingpt_sentiment_checkpoint = import_csv_to_table(\"fingpt_sentiment_checkpoint\", os.path.join(processed_data_path, \"fingpt_sentiment_checkpoint.csv\"), False)\n",
    "lstm_predictions = import_csv_to_table(\"lstm_predictions\", os.path.join(processed_data_path, \"lstm_predictions.csv\"), False)\n",
    "\n",
    "print(\"All tables imported and SQL views created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93893c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+-------------------+--------------------+----------------+----------------+-------------------+-----------------+------------+------------------+--------------------+\n",
      "|symbol|split|    prediction_date|        target_date|       actual_return|predicted_return|actual_direction|predicted_direction|direction_correct|actual_price|   predicted_price|symbol_test_accuracy|\n",
      "+------+-----+-------------------+-------------------+--------------------+----------------+----------------+-------------------+-----------------+------------+------------------+--------------------+\n",
      "|     A|train|2025-05-19 04:00:00|2025-05-27 04:00:00|-0.01921720733427356|    -0.012971029|               0|                  0|                1|      111.26|111.96856481552123|             0.59375|\n",
      "|     A|train|2025-05-20 04:00:00|2025-05-28 04:00:00|-0.02291152626013402|   -0.0139999315|               0|                  0|                1|      110.88|111.89128692626953|             0.59375|\n",
      "|     A|train|2025-05-21 04:00:00|2025-05-29 04:00:00| 0.04012487374896708|    -0.014272444|               1|                  0|                0|      113.28|107.35558731794357|             0.59375|\n",
      "|     A|train|2025-05-22 04:00:00|2025-05-30 04:00:00| 0.02042304886943832|    -0.015078142|               1|                  0|                0|      111.92| 108.0262309885025|             0.59375|\n",
      "|     A|train|2025-05-23 04:00:00|2025-06-02 04:00:00| 0.02321938634478942|     -0.01604934|               1|                  0|                0|      111.05|106.78816670715808|             0.59375|\n",
      "|     A|train|2025-05-27 04:00:00|2025-06-03 04:00:00|0.013481934208161064|    -0.018028468|               1|                  0|                0|      112.76|109.25414934635162|             0.59375|\n",
      "|     A|train|2025-05-28 04:00:00|2025-06-04 04:00:00| 0.03661616161616164|     -0.01993908|               1|                  0|                0|      114.94|108.66915647506714|             0.59375|\n",
      "|     A|train|2025-05-29 04:00:00|2025-06-05 04:00:00|0.005826271186440648|    -0.023287162|               1|                  0|                0|      113.94|110.64202857971192|             0.59375|\n",
      "|     A|train|2025-05-30 04:00:00|2025-06-06 04:00:00| 0.03466761972837737|     -0.02644106|               1|                  0|                0|       115.8| 108.9607190656662|             0.59375|\n",
      "|     A|train|2025-06-02 04:00:00|2025-06-09 04:00:00|0.051238180999549736|    -0.030219086|               1|                  0|                0|      116.74| 107.6941713809967|             0.59375|\n",
      "|     A|train|2025-06-03 04:00:00|2025-06-10 04:00:00| 0.06881873004611556|    -0.034516037|               1|                  0|                0|      120.52|108.86797172307969|             0.59375|\n",
      "|     A|train|2025-06-04 04:00:00|2025-06-11 04:00:00| 0.04158691491212808|     -0.03892179|               1|                  0|                0|      119.72|110.46633136153221|             0.59375|\n",
      "|     A|train|2025-06-05 04:00:00|2025-06-12 04:00:00| 0.04361944883271897|    -0.042293802|               1|                  0|                0|      118.91|109.12104590892791|             0.59375|\n",
      "|     A|train|2025-06-06 04:00:00|2025-06-13 04:00:00|0.011053540587219354|    -0.044976704|               1|                  0|                0|      117.08| 110.5916968345642|             0.59375|\n",
      "|     A|train|2025-06-09 04:00:00|2025-06-16 04:00:00|0.019530580777796826|      -0.0475979|               1|                  0|                0|      119.02|111.18342288970946|             0.59375|\n",
      "|     A|train|2025-06-10 04:00:00|2025-06-17 04:00:00|-0.03675738466644534|    -0.051105507|               0|                  0|                1|      116.09|114.36076522827148|             0.59375|\n",
      "|     A|train|2025-06-11 04:00:00|2025-06-18 04:00:00|-0.03508185766789177|    -0.054937996|               0|                  0|                1|      115.52|113.14282044649124|             0.59375|\n",
      "|     A|train|2025-06-12 04:00:00|2025-06-20 04:00:00| -0.0281725674880161|    -0.059035785|               0|                  0|                1|      115.56|111.89005563259124|             0.59375|\n",
      "|     A|train|2025-06-13 04:00:00|2025-06-23 04:00:00|-0.00811411001024...|    -0.062332667|               0|                  0|                1|      116.13|109.78208868265152|             0.59375|\n",
      "|     A|train|2025-06-16 04:00:00|2025-06-24 04:00:00|-0.01159468996807...|     -0.06601269|               0|                  0|                1|      117.64|111.16317076325416|             0.59375|\n",
      "+------+-----+-------------------+-------------------+--------------------+----------------+----------------+-------------------+-----------------+------------+------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "lstm_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee188a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
