{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96eec700",
   "metadata": {},
   "source": [
    "# LSTM Stock Price Prediction\n",
    "Predict next-day close prices using LSTM neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Preview Data\n",
    "df = pd.read_csv('raw_data/stock_data.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.drop(columns=['index'], errors='ignore')  # Drop useless index column\n",
    "df = df.sort_values(['symbol', 'timestamp']).reset_index(drop=True)\n",
    "print(f\"Shape: {df.shape}, Symbols: {df['symbol'].nunique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Feature Engineering - Multi-Scale Technical Indicators\n",
    "def add_technical_indicators(group):\n",
    "    \"\"\"Add multi-scale technical indicators for pattern recognition\"\"\"\n",
    "    g = group.copy()\n",
    "    \n",
    "    # Price structure (normalized)\n",
    "    g['price_range'] = (g['high'] - g['low']) / g['close']\n",
    "    g['body_size'] = abs(g['close'] - g['open']) / g['close']  # Candle body\n",
    "    g['upper_shadow'] = (g['high'] - g[['open', 'close']].max(axis=1)) / g['close']\n",
    "    g['lower_shadow'] = (g[['open', 'close']].min(axis=1) - g['low']) / g['close']\n",
    "    \n",
    "    # Multi-scale Moving Averages (% distance from price)\n",
    "    for period in [5, 10, 20, 50]:\n",
    "        g[f'sma_{period}'] = g['close'].rolling(period).mean() / g['close'] - 1\n",
    "    \n",
    "    # Multi-scale EMAs\n",
    "    for period in [8, 13, 21, 34]:  # Fibonacci-like\n",
    "        g[f'ema_{period}'] = g['close'].ewm(span=period, adjust=False).mean() / g['close'] - 1\n",
    "    \n",
    "    # Multi-scale RSI\n",
    "    for period in [7, 14, 21]:\n",
    "        delta = g['close'].diff()\n",
    "        gain = delta.clip(lower=0).rolling(period).mean()\n",
    "        loss = (-delta.clip(upper=0)).rolling(period).mean()\n",
    "        g[f'rsi_{period}'] = 100 - (100 / (1 + gain / (loss + 1e-10)))\n",
    "    \n",
    "    # MACD with signal line\n",
    "    ema12 = g['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = g['close'].ewm(span=26, adjust=False).mean()\n",
    "    macd_line = ema12 - ema26\n",
    "    g['macd'] = macd_line / g['close']\n",
    "    g['macd_signal'] = macd_line.ewm(span=9, adjust=False).mean() / g['close']\n",
    "    g['macd_hist'] = (macd_line - macd_line.ewm(span=9, adjust=False).mean()) / g['close']\n",
    "    \n",
    "    # Bollinger Bands (multiple)\n",
    "    for period in [10, 20]:\n",
    "        sma = g['close'].rolling(period).mean()\n",
    "        std = g['close'].rolling(period).std()\n",
    "        g[f'bb_pos_{period}'] = (g['close'] - sma) / (2 * std + 1e-10)\n",
    "        g[f'bb_width_{period}'] = (4 * std) / g['close']  # Band width = volatility\n",
    "    \n",
    "    # Multi-scale volatility (ATR)\n",
    "    for period in [7, 14, 21]:\n",
    "        g[f'atr_{period}'] = g['price_range'].rolling(period).mean()\n",
    "    \n",
    "    # Volume patterns\n",
    "    g['volume_sma_10'] = g['volume'].rolling(10).mean() / (g['volume'] + 1e-10) - 1\n",
    "    g['volume_sma_20'] = g['volume'].rolling(20).mean() / (g['volume'] + 1e-10) - 1\n",
    "    g['vwap_diff'] = (g['vwap'] - g['close']) / g['close']\n",
    "    \n",
    "    # Price momentum (rate of change) - multi-scale\n",
    "    for period in [5, 10, 20]:\n",
    "        g[f'roc_{period}'] = g['close'].pct_change(period)\n",
    "    \n",
    "    # Mean reversion signal\n",
    "    g['close_zscore'] = (g['close'] - g['close'].rolling(20).mean()) / (g['close'].rolling(20).std() + 1e-10)\n",
    "    \n",
    "    # Drop warmup rows (need 50 days for longest indicator)\n",
    "    return g.iloc[50:]\n",
    "\n",
    "# Apply per-symbol\n",
    "print(\"Adding multi-scale technical indicators...\")\n",
    "df_list = []\n",
    "for symbol in df['symbol'].unique():\n",
    "    sym_df = df[df['symbol'] == symbol].copy()\n",
    "    sym_df = add_technical_indicators(sym_df)\n",
    "    df_list.append(sym_df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"NaN counts:\\n{df.isna().sum()[df.isna().sum() > 0]}\")\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nAfter feature engineering: {df.shape}, Symbols: {df['symbol'].nunique()}\")\n",
    "print(f\"Features created: {len([c for c in df.columns if c not in ['symbol', 'timestamp', 'open', 'high', 'low', 'close', 'volume', 'vwap', 'trade_count', 'returns', 'log_returns', 'gap']])}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56137bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configuration - Tuned for better accuracy\n",
    "SEQUENCE_LENGTH = 40\n",
    "PREDICTION_HORIZON = 5\n",
    "HIDDEN_SIZE = 256  # Increased capacity\n",
    "\n",
    "# Multi-scale feature set (no return leakage)\n",
    "FEATURES = [\n",
    "    # Price structure\n",
    "    'price_range', 'body_size', 'upper_shadow', 'lower_shadow',\n",
    "    # Multi-scale SMAs\n",
    "    'sma_5', 'sma_10', 'sma_20', 'sma_50',\n",
    "    # Multi-scale EMAs\n",
    "    'ema_8', 'ema_13', 'ema_21', 'ema_34',\n",
    "    # Multi-scale RSI\n",
    "    'rsi_7', 'rsi_14', 'rsi_21',\n",
    "    # MACD components\n",
    "    'macd', 'macd_signal', 'macd_hist',\n",
    "    # Bollinger bands\n",
    "    'bb_pos_10', 'bb_width_10', 'bb_pos_20', 'bb_width_20',\n",
    "    # Multi-scale volatility\n",
    "    'atr_7', 'atr_14', 'atr_21',\n",
    "    # Volume\n",
    "    'volume_sma_10', 'volume_sma_20', 'vwap_diff',\n",
    "    # Momentum (ROC)\n",
    "    'roc_5', 'roc_10', 'roc_20',\n",
    "    # Mean reversion\n",
    "    'close_zscore'\n",
    "]\n",
    "\n",
    "TARGET = 'returns'\n",
    "TRAIN_SPLIT = 0.8\n",
    "EPOCHS = 300  # More training\n",
    "PATIENCE = 35  # More patience\n",
    "DROPOUT = 0.15  # Less regularization\n",
    "LEARNING_RATE = 0.0005  # Lower LR for stability\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}, Features: {len(FEATURES)}, Hidden: {HIDDEN_SIZE}, LR: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Preparation - predict N-day cumulative return\n",
    "def prepare_sequences(data, raw_data, seq_len, features, horizon=5):\n",
    "    \"\"\"Create sequences for N-day ahead prediction\"\"\"\n",
    "    X, y, prev_close = [], [], []\n",
    "    feature_data = data[features].values\n",
    "    close_prices = raw_data['close'].values\n",
    "    \n",
    "    # Calculate N-day cumulative return\n",
    "    for i in range(len(data) - seq_len - horizon + 1):\n",
    "        X.append(feature_data[i:i+seq_len])\n",
    "        # Target: return from day (seq_len-1) to day (seq_len + horizon - 1)\n",
    "        future_close = close_prices[i + seq_len + horizon - 1]\n",
    "        current_close = close_prices[i + seq_len - 1]\n",
    "        cumulative_return = (future_close - current_close) / current_close\n",
    "        y.append(cumulative_return)\n",
    "        prev_close.append(current_close)\n",
    "    \n",
    "    return np.array(X), np.array(y), np.array(prev_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba788f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: LSTM with Self-Attention\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Bidirectional LSTM for richer patterns\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # Self-attention over timesteps\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.bn = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM: (batch, seq, features) -> (batch, seq, hidden*2)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Attention weights: (batch, seq, 1)\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        \n",
    "        # Weighted sum: (batch, hidden*2)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Output\n",
    "        context = self.bn(context)\n",
    "        return self.fc(context)\n",
    "\n",
    "# Alias for compatibility\n",
    "LSTMPredictor = AttentionLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Custom Loss Function + Training with Directional Penalty\n",
    "class DirectionalMSELoss(nn.Module):\n",
    "    \"\"\"MSE + penalty for wrong direction predictions\"\"\"\n",
    "    def __init__(self, direction_weight=2.0):\n",
    "        super().__init__()\n",
    "        self.direction_weight = direction_weight\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        mse_loss = self.mse(pred, target)\n",
    "        \n",
    "        # Directional penalty: extra cost when sign is wrong\n",
    "        pred_sign = torch.sign(pred)\n",
    "        target_sign = torch.sign(target)\n",
    "        wrong_direction = (pred_sign != target_sign).float()\n",
    "        \n",
    "        # Penalize wrong directions more heavily\n",
    "        directional_penalty = wrong_direction * torch.abs(target) * self.direction_weight\n",
    "        \n",
    "        return (mse_loss + directional_penalty).mean()\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=150, batch_size=32, lr=0.001, patience=20, verbose=True):\n",
    "    criterion = DirectionalMSELoss(direction_weight=2.0)  # Custom loss\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    X_t = torch.FloatTensor(X_train).to(DEVICE)\n",
    "    y_t = torch.FloatTensor(y_train).reshape(-1, 1).to(DEVICE)\n",
    "    X_v = torch.FloatTensor(X_val).to(DEVICE)\n",
    "    y_v = torch.FloatTensor(y_val).reshape(-1, 1).to(DEVICE)\n",
    "    \n",
    "    best_val_loss, patience_counter, best_state = float('inf'), 0, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        # Shuffle training data\n",
    "        indices = torch.randperm(len(X_t))\n",
    "        for i in range(0, len(X_t), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            batch_X, batch_y = X_t[batch_idx], y_t[batch_idx]\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(batch_X), batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(X_v), y_v).item()\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, patience_counter = val_loss, 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if verbose and (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train: {total_loss:.4f}, Val: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9722b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Process All Symbols and Store Results\n",
    "results = {}\n",
    "scalers = {}\n",
    "models = {}\n",
    "\n",
    "# Use ALL symbols (filter out those with insufficient data)\n",
    "min_required = SEQUENCE_LENGTH + PREDICTION_HORIZON + 30\n",
    "symbol_counts = df.groupby('symbol').size()\n",
    "top_symbols = symbol_counts[symbol_counts >= min_required].index.tolist()\n",
    "print(f\"Training on {len(top_symbols)} symbols (all with >= {min_required} data points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train Models with tuned hyperparameters\n",
    "for symbol in tqdm(top_symbols, desc=\"Training symbols\"):\n",
    "    sym_data = df[df['symbol'] == symbol].copy()\n",
    "    raw_data = sym_data.copy()\n",
    "    \n",
    "    if len(sym_data) < SEQUENCE_LENGTH + PREDICTION_HORIZON + 30:\n",
    "        continue\n",
    "    \n",
    "    # Store timestamps for later export\n",
    "    timestamps = raw_data['timestamp'].values\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    sym_data[FEATURES] = scaler.fit_transform(sym_data[FEATURES])\n",
    "    scalers[symbol] = scaler\n",
    "    \n",
    "    # Create sequences with N-day horizon\n",
    "    X, y, prev_close = prepare_sequences(sym_data, raw_data, SEQUENCE_LENGTH, FEATURES, PREDICTION_HORIZON)\n",
    "    split = int(len(X) * TRAIN_SPLIT)\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    prev_close_train, prev_close_test = prev_close[:split], prev_close[split:]\n",
    "    \n",
    "    # Get actual future close prices\n",
    "    all_close = raw_data['close'].values[SEQUENCE_LENGTH + PREDICTION_HORIZON - 1:\n",
    "                                          SEQUENCE_LENGTH + PREDICTION_HORIZON - 1 + len(y)]\n",
    "    y_close_train, y_close_test = all_close[:split], all_close[split:]\n",
    "    \n",
    "    # Get dates for train and test\n",
    "    all_pred_dates = timestamps[np.arange(len(y)) + SEQUENCE_LENGTH - 1]\n",
    "    all_target_dates = timestamps[np.arange(len(y)) + SEQUENCE_LENGTH + PREDICTION_HORIZON - 1]\n",
    "    train_pred_dates, test_pred_dates = all_pred_dates[:split], all_pred_dates[split:]\n",
    "    train_target_dates, test_target_dates = all_target_dates[:split], all_target_dates[split:]\n",
    "    \n",
    "    # Train/val split\n",
    "    val_split = int(len(X_train) * 0.9)\n",
    "    X_tr, X_val = X_train[:val_split], X_train[val_split:]\n",
    "    y_tr, y_val = y_train[:val_split], y_train[val_split:]\n",
    "    \n",
    "    # Train model with TUNED params (silent mode)\n",
    "    model = LSTMPredictor(len(FEATURES), hidden_size=HIDDEN_SIZE, dropout=DROPOUT).to(DEVICE)\n",
    "    train_model(model, X_tr, y_tr, X_val, y_val, epochs=EPOCHS, patience=PATIENCE, lr=LEARNING_RATE, verbose=False)\n",
    "    models[symbol] = model\n",
    "    \n",
    "    # Predict returns for BOTH train and test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred_returns = model(torch.FloatTensor(X_train).to(DEVICE)).cpu().numpy().flatten()\n",
    "        test_pred_returns = model(torch.FloatTensor(X_test).to(DEVICE)).cpu().numpy().flatten()\n",
    "    \n",
    "    # Convert predicted returns to prices\n",
    "    train_pred_prices = prev_close_train * (1 + train_pred_returns)\n",
    "    test_pred_prices = prev_close_test * (1 + test_pred_returns)\n",
    "    \n",
    "    # Metrics (test only)\n",
    "    mse = mean_squared_error(y_close_test, test_pred_prices)\n",
    "    mae = mean_absolute_error(y_close_test, test_pred_prices)\n",
    "    \n",
    "    # Directional accuracy (test only)\n",
    "    actual_dir = y_test > 0\n",
    "    pred_dir = test_pred_returns > 0\n",
    "    dir_acc = (actual_dir == pred_dir).mean() * 100\n",
    "    \n",
    "    results[symbol] = {\n",
    "        'mse': mse, 'mae': mae, 'dir_acc': dir_acc,\n",
    "        # Test set\n",
    "        'y_test': y_close_test, 'preds': test_pred_prices,\n",
    "        'y_returns': y_test, 'pred_returns': test_pred_returns,\n",
    "        'pred_dates': test_pred_dates, 'target_dates': test_target_dates,\n",
    "        # Train set\n",
    "        'y_train_close': y_close_train, 'train_preds': train_pred_prices,\n",
    "        'y_train_returns': y_train, 'train_pred_returns': train_pred_returns,\n",
    "        'train_pred_dates': train_pred_dates, 'train_target_dates': train_target_dates\n",
    "    }\n",
    "\n",
    "print(f\"\\nCompleted training {len(results)} symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ad1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Results Summary\n",
    "results_df = pd.DataFrame({\n",
    "    k: {'MAE ($)': v['mae'], 'RMSE ($)': np.sqrt(v['mse']), 'Direction Acc (%)': v['dir_acc']} \n",
    "    for k, v in results.items()\n",
    "}).T\n",
    "print(\"\\nModel Performance Summary (Predicting % Returns â†’ Prices):\")\n",
    "print(f\"Average Direction Accuracy: {results_df['Direction Acc (%)'].mean():.1f}%\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualize Price Predictions\n",
    "fig, axes = plt.subplots(len(results), 1, figsize=(14, 4*len(results)))\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (symbol, res) in zip(axes, results.items()):\n",
    "    ax.plot(res['y_test'], label='Actual', alpha=0.8, linewidth=1.5)\n",
    "    ax.plot(res['preds'], label='Predicted', alpha=0.8, linewidth=1.5)\n",
    "    ax.set_title(f'{symbol} | MAE: ${res[\"mae\"]:.2f} | Dir Acc: {res[\"dir_acc\"]:.1f}%')\n",
    "    ax.set_ylabel('Price ($)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe669c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Visualize Return Predictions (shows directional accuracy better)\n",
    "fig, axes = plt.subplots(len(results), 1, figsize=(14, 3*len(results)))\n",
    "if len(results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (symbol, res) in zip(axes, results.items()):\n",
    "    x = range(len(res['y_returns']))\n",
    "    ax.bar(x, res['y_returns']*100, alpha=0.5, label='Actual %', width=0.8)\n",
    "    ax.plot(x, res['pred_returns']*100, 'r-', alpha=0.8, linewidth=1.5, label='Predicted %')\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.set_title(f'{symbol} - Daily Returns | Dir Acc: {res[\"dir_acc\"]:.1f}%')\n",
    "    ax.set_ylabel('Return (%)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Export All Predictions with Train/Test Split Indicator\n",
    "\n",
    "dfs = []\n",
    "for symbol, res in results.items():\n",
    "    # Test set\n",
    "    test_df = pd.DataFrame({\n",
    "        'symbol': symbol,\n",
    "        'split': 'test',\n",
    "        'prediction_date': res['pred_dates'],\n",
    "        'target_date': res['target_dates'],\n",
    "        'actual_return': res['y_returns'],\n",
    "        'predicted_return': res['pred_returns'],\n",
    "        'actual_direction': (res['y_returns'] > 0).astype(int),\n",
    "        'predicted_direction': (res['pred_returns'] > 0).astype(int),\n",
    "        'direction_correct': ((res['y_returns'] > 0) == (res['pred_returns'] > 0)).astype(int),\n",
    "        'actual_price': res['y_test'],\n",
    "        'predicted_price': res['preds'],\n",
    "        'symbol_test_accuracy': res['dir_acc'] / 100\n",
    "    })\n",
    "    \n",
    "    # Train set\n",
    "    train_df = pd.DataFrame({\n",
    "        'symbol': symbol,\n",
    "        'split': 'train',\n",
    "        'prediction_date': res['train_pred_dates'],\n",
    "        'target_date': res['train_target_dates'],\n",
    "        'actual_return': res['y_train_returns'],\n",
    "        'predicted_return': res['train_pred_returns'],\n",
    "        'actual_direction': (res['y_train_returns'] > 0).astype(int),\n",
    "        'predicted_direction': (res['train_pred_returns'] > 0).astype(int),\n",
    "        'direction_correct': ((res['y_train_returns'] > 0) == (res['train_pred_returns'] > 0)).astype(int),\n",
    "        'actual_price': res['y_train_close'],\n",
    "        'predicted_price': res['train_preds'],\n",
    "        'symbol_test_accuracy': res['dir_acc'] / 100  # Still test accuracy for reference\n",
    "    })\n",
    "    \n",
    "    dfs.append(test_df)\n",
    "    dfs.append(train_df)\n",
    "\n",
    "predictions_df = pd.concat(dfs, ignore_index=True)\n",
    "predictions_df = predictions_df.sort_values(['symbol', 'prediction_date']).reset_index(drop=True)\n",
    "predictions_df.to_csv('processed_data/lstm_predictions.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(predictions_df)} predictions to processed_data/lstm_predictions.csv\")\n",
    "print(predictions_df.groupby(['symbol', 'split'])['direction_correct'].agg(['mean', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Save Full Model Bundle (all symbols)\n",
    "import pickle\n",
    "\n",
    "# Save all models, scalers, and config in one bundle\n",
    "model_bundle = {\n",
    "    'models': {sym: model.state_dict() for sym, model in models.items()},\n",
    "    'scalers': scalers,\n",
    "    'config': {\n",
    "        'FEATURES': FEATURES,\n",
    "        'SEQUENCE_LENGTH': SEQUENCE_LENGTH,\n",
    "        'PREDICTION_HORIZON': PREDICTION_HORIZON,\n",
    "        'HIDDEN_SIZE': HIDDEN_SIZE,\n",
    "        'DROPOUT': DROPOUT,\n",
    "        'symbols': list(models.keys())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as pickle\n",
    "with open('model_archive/lstm_model_bundle.pkl', 'wb') as f:\n",
    "    pickle.dump(model_bundle, f)\n",
    "\n",
    "# Save individual model weights\n",
    "for symbol, model in models.items():\n",
    "    torch.save(model.state_dict(), f'model_archive/lstm_{symbol}.pt')\n",
    "\n",
    "print(f\"Saved model bundle with {len(models)} symbols to model_archive/lstm_model_bundle.pkl\")\n",
    "print(f\"Individual models saved: {list(models.keys())}\")\n",
    "\n",
    "# Save accuracy summary (test set only)\n",
    "accuracy_df = predictions_df[predictions_df['split'] == 'test'].groupby('symbol').agg({\n",
    "    'direction_correct': 'mean'\n",
    "}).rename(columns={'direction_correct': 'test_accuracy'})\n",
    "accuracy_df.to_csv('processed_data/lstm_accuracy.csv')\n",
    "print(f\"\\nAccuracy saved to processed_data/lstm_accuracy.csv\")\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ecf5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
